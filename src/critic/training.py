"""Critic training: learn routing success prediction.

Training data: (partial routing state, final routing score) pairs
Generated by running nextpnr router on partial routings.

This is NOT a heuristic - it's learned from ground truth.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from typing import List, Tuple, Dict, Optional
from dataclasses import dataclass

from .gnn import RoutingCritic, RoutingGraph
from .features import RoutingGraphBuilder
from ..diffusion.model import RoutingState
from ..core.routing.grid import Grid
from ..core.routing.netlist import Netlist


@dataclass
class RoutingExample:
    """Training example for routing critic.

    Contains partial routing state and ground-truth outcome.
    """
    state: RoutingState
    netlist: Netlist
    grid: Grid
    final_score: float  # 0-1, from actual router
    failed_nets: int    # Number of nets that failed to route
    congestion_max: float
    timing_slack: float


class RoutingDataset(Dataset):
    """Dataset of routing states and outcomes."""

    def __init__(
        self,
        examples: List[RoutingExample],
        graph_builder: RoutingGraphBuilder
    ):
        self.examples = examples
        self.graph_builder = graph_builder

    def __len__(self) -> int:
        return len(self.examples)

    def __getitem__(self, idx: int) -> Tuple[RoutingGraph, float]:
        ex = self.examples[idx]
        graph = self.graph_builder.build_graph(ex.state, ex.netlist)
        return graph, ex.final_score


def collate_routing_graphs(
    batch: List[Tuple[RoutingGraph, float]]
) -> Tuple[RoutingGraph, torch.Tensor]:
    """Collate routing graphs into batch."""
    graphs, scores = zip(*batch)

    # Concatenate node features
    node_features = torch.cat([g.node_features for g in graphs], dim=0)

    # Offset edge indices
    edge_indices = []
    offset = 0
    for g in graphs:
        edge_indices.append(g.edge_index + offset)
        offset += g.node_features.size(0)
    edge_index = torch.cat(edge_indices, dim=1) if edge_indices else torch.zeros(2, 0, dtype=torch.long)

    # Concatenate other features
    edge_features = torch.cat([g.edge_features for g in graphs], dim=0)
    congestion = torch.cat([g.congestion for g in graphs], dim=0)
    unrouted_mask = torch.cat([g.unrouted_mask for g in graphs], dim=0)

    # Batch assignment
    batch_idx = torch.cat([
        torch.full((g.node_features.size(0),), i, dtype=torch.long)
        for i, g in enumerate(graphs)
    ])

    batched_graph = RoutingGraph(
        node_features=node_features,
        edge_index=edge_index,
        edge_features=edge_features,
        congestion=congestion,
        unrouted_mask=unrouted_mask,
        batch=batch_idx
    )

    return batched_graph, torch.tensor(scores, dtype=torch.float32)


class CriticTrainer:
    """Trainer for routing critic.

    Training loop:
    1. Sample partial routing states (from diffusion trajectories)
    2. Complete routing with nextpnr
    3. Record (partial state, final score) pairs
    4. Train critic with MSE loss
    """

    def __init__(
        self,
        critic: RoutingCritic,
        lr: float = 1e-4,
        weight_decay: float = 1e-5,
        device: str = "cuda"
    ):
        self.critic = critic.to(device)
        self.device = device

        self.optimizer = optim.AdamW(
            critic.parameters(),
            lr=lr,
            weight_decay=weight_decay
        )

        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=1000, eta_min=1e-6
        )

        # MSE loss for value prediction
        self.criterion = nn.MSELoss()

    def train_epoch(
        self,
        dataloader: DataLoader,
        log_interval: int = 100
    ) -> Dict[str, float]:
        """Train for one epoch."""
        self.critic.train()

        total_loss = 0.0
        total_mae = 0.0
        num_batches = 0

        for batch_idx, (graph, targets) in enumerate(dataloader):
            # Move to device
            graph = RoutingGraph(
                node_features=graph.node_features.to(self.device),
                edge_index=graph.edge_index.to(self.device),
                edge_features=graph.edge_features.to(self.device),
                congestion=graph.congestion.to(self.device),
                unrouted_mask=graph.unrouted_mask.to(self.device),
                batch=graph.batch.to(self.device) if graph.batch is not None else None
            )
            targets = targets.to(self.device)

            # Forward
            self.optimizer.zero_grad()
            predictions = self.critic(graph)

            # Loss
            loss = self.criterion(predictions, targets)

            # Backward
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)
            self.optimizer.step()

            # Track metrics
            total_loss += loss.item()
            total_mae += (predictions - targets).abs().mean().item()
            num_batches += 1

            if (batch_idx + 1) % log_interval == 0:
                print(f"  Batch {batch_idx + 1}: loss={loss.item():.4f}")

        self.scheduler.step()

        return {
            "loss": total_loss / num_batches,
            "mae": total_mae / num_batches
        }

    def evaluate(self, dataloader: DataLoader) -> Dict[str, float]:
        """Evaluate on validation set."""
        self.critic.eval()

        total_loss = 0.0
        total_mae = 0.0
        num_batches = 0

        # For threshold analysis
        predictions_all = []
        targets_all = []

        with torch.no_grad():
            for graph, targets in dataloader:
                graph = RoutingGraph(
                    node_features=graph.node_features.to(self.device),
                    edge_index=graph.edge_index.to(self.device),
                    edge_features=graph.edge_features.to(self.device),
                    congestion=graph.congestion.to(self.device),
                    unrouted_mask=graph.unrouted_mask.to(self.device),
                    batch=graph.batch.to(self.device) if graph.batch is not None else None
                )
                targets = targets.to(self.device)

                predictions = self.critic(graph)

                loss = self.criterion(predictions, targets)
                total_loss += loss.item()
                total_mae += (predictions - targets).abs().mean().item()
                num_batches += 1

                predictions_all.extend(predictions.cpu().tolist())
                targets_all.extend(targets.cpu().tolist())

        # Compute pruning metrics at different thresholds
        predictions_t = torch.tensor(predictions_all)
        targets_t = torch.tensor(targets_all)

        metrics = {
            "loss": total_loss / num_batches,
            "mae": total_mae / num_batches
        }

        for threshold in [0.3, 0.5, 0.7]:
            # How many would we prune?
            pruned = (predictions_t < threshold).float().mean().item()

            # Of those pruned, how many were actually bad?
            actually_bad = (targets_t < threshold).float()
            predicted_bad = (predictions_t < threshold).float()

            # Precision: of predicted bad, how many are actually bad
            if predicted_bad.sum() > 0:
                precision = (predicted_bad * actually_bad).sum() / predicted_bad.sum()
            else:
                precision = 1.0

            # Recall: of actually bad, how many did we catch
            if actually_bad.sum() > 0:
                recall = (predicted_bad * actually_bad).sum() / actually_bad.sum()
            else:
                recall = 1.0

            metrics[f"prune_rate_{threshold}"] = pruned
            metrics[f"precision_{threshold}"] = precision.item()
            metrics[f"recall_{threshold}"] = recall.item()

        return metrics

    def save_checkpoint(self, path: str) -> None:
        """Save checkpoint."""
        torch.save({
            "model_state_dict": self.critic.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "scheduler_state_dict": self.scheduler.state_dict()
        }, path)

    def load_checkpoint(self, path: str) -> None:
        """Load checkpoint."""
        checkpoint = torch.load(path, map_location=self.device)
        self.critic.load_state_dict(checkpoint["model_state_dict"])
        self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        self.scheduler.load_state_dict(checkpoint["scheduler_state_dict"])


def generate_training_data(
    netlists: List[Netlist],
    grids: List[Grid],
    router_fn,  # Function: (state, netlist, grid) -> score
    diffusion_model,  # For generating partial states
    samples_per_netlist: int = 100
) -> List[RoutingExample]:
    """Generate training data by sampling partial routings.

    1. Start from noise
    2. Denoise to various timesteps (partial routings)
    3. Complete with real router
    4. Record (partial, final_score) pairs
    """
    examples = []

    for netlist, grid in zip(netlists, grids):
        for _ in range(samples_per_netlist):
            # Sample random partial routing via diffusion
            # ... (implementation depends on diffusion model interface)
            pass

    return examples
