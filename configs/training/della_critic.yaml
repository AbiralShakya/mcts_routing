# Della cluster-specific critic training configuration

# Cluster settings
cluster:
  name: "della"
  partition: "gpu"
  num_gpus: 1
  nodes: 1

# Distributed training (disabled for single-GPU)
distributed:
  enabled: false
  backend: "nccl"
  init_method: "env://"

# Critic model
model:
  node_dim: 64
  edge_dim: 32
  hidden_dim: 128
  num_layers: 4
  dropout: 0.1
  net_feat_dim: 7  # Must match diffusion model net_feat_dim
  
# Shared encoder settings
shared_encoders:
  use_shared_encoders: true  # Set to true to use shared encoders from diffusion model
  diffusion_checkpoint: null  # Path to diffusion checkpoint to load shared encoders from
  # If null, will create new shared encoders
  # If provided, will load encoder weights from diffusion checkpoint

# Training
training:
  batch_size: 16  # Smaller batch for single GPU
  num_epochs: 100
  learning_rate: 1.0e-4
  weight_decay: 1.0e-5

# Data
data:
  data_dir: "${SCRATCH}/data/critic_data"
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1

optimizer:
  type: "adamw"
  betas: [0.9, 0.999]

scheduler:
  type: "cosine"
  T_max: 100
  eta_min: 1.0e-6

# Checkpointing
checkpointing:
  checkpoint_dir: "${SCRATCH}/checkpoints/critic"
  save_every_n_epochs: 10
  keep_last_n: 5

# Logging
log_interval: 100

