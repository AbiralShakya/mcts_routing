# Training base configuration
training:
  batch_size: 32
  num_epochs: 100
  learning_rate: 1e-4
  weight_decay: 1e-5
  gradient_clip_norm: 1.0
  
optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  
lr_schedule:
  type: "cosine"
  warmup_steps: 1000
  
loss:
  diffusion_loss_weight: 1.0
  lipschitz_reg_weight: 0.01
  identifiability_reg_weight: 0.001
  
checkpointing:
  save_every_n_epochs: 10
  keep_last_n: 5

