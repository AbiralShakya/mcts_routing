# Della cluster-specific routing diffusion training configuration
# Extends routing_diffusion.yaml

# Cluster settings
cluster:
  name: "della"
  partition: "gpu"
  num_gpus: 1
  nodes: 1

# Distributed training (disabled for single-GPU)
distributed:
  enabled: false
  backend: "nccl"
  init_method: "env://"

# Model (same as base)
model:
  num_timesteps: 1000
  hidden_dim: 256
  num_layers: 6
  max_pips_per_net: 1000
  num_heads: 8
  dropout: 0.1
  net_feat_dim: 7  # Dimension of net feature vectors
  
# Shared encoder settings (for joint training with critic)
shared_encoders:
  enabled: true  # Set to true to use shared encoders
  # If enabled, encoders will be created and can be shared with critic model

# Training (single-GPU batch size)
training:
  batch_size: 32  # Smaller batch for single GPU
  num_epochs: 200
  learning_rate: 1.0e-4
  weight_decay: 1.0e-5
  gradient_clip_norm: 1.0

# Data (use $SCRATCH)
data:
  data_dir: "${SCRATCH}/data/routing_states"
  train_split: 0.9
  val_split: 0.1

optimizer:
  type: "adamw"
  betas: [0.9, 0.999]

scheduler:
  type: "cosine"
  T_max: 200

# Checkpointing (use $SCRATCH)
checkpointing:
  checkpoint_dir: "${SCRATCH}/checkpoints"
  save_every_n_epochs: 10
  keep_last_n: 5

